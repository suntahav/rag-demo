{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-31T21:47:40.088224400Z",
     "start_time": "2023-12-31T21:47:37.801431500Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sands\\PycharmProjects\\rag-demo\\venv\\lib\\site-packages\\pinecone\\index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from db_init import *"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Please dont run this cell again as trial pinecone account allows only 1 index which is already created"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "57d0fc9970bc09bc"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document search initialized\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# url = 'https://arxiv.org/pdf/2005.11401.pdf'\n",
    "url = \"2005.11401\"\n",
    "initdb(url)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T04:36:44.881960500Z",
     "start_time": "2023-12-31T04:36:34.596316500Z"
    }
   },
   "id": "c60f64b5e575097"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Please put the url api endpoint here"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "113634f6be6d452e"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "url = \"URL API ENDPOINT\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T22:02:23.850328800Z",
     "start_time": "2023-12-31T22:02:23.836401600Z"
    }
   },
   "id": "82cc729f9197ae00"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sample Runs for API"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c198b7b71c2e2acc"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper \"RAG: Retrieval-Augmented Generation for Knowledge-Intensive Tasks\" proposes a method called Retrieval-Augmented Generation (RAG) for improving performance on knowledge-intensive tasks. The method utilizes a retrieval-based approach to gather information from a large collection of documents and combines it with a language model to generate more accurate and informative responses. The experiments section of the paper evaluates the RAG approach on open-domain question-answering tasks and shows that it outperforms traditional approaches and baselines. The paper concludes by discussing the implications of the RAG method for improving performance on knowledge-intensive tasks and suggesting future directions for research. \n",
      "\n",
      "Would you like me to extract any specific information from the paper?\n"
     ]
    }
   ],
   "source": [
    "request_body = {\n",
    "    \"userprompt\": \"Can you summarize the document in 100 words?\"\n",
    "}\n",
    "response = requests.post(url, json=request_body)\n",
    "print(response.json())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T21:49:58.618431300Z",
     "start_time": "2023-12-31T21:49:51.710723800Z"
    }
   },
   "id": "65dbb1169bad3f83"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, I'd be happy to explain the training procedure for the RAG model!\n",
      "\n",
      "The RAG model, which stands for Retrieval-Augmented Generation, is trained using the Fairseq library with mixed precision floating point arithmetic on 8 NVIDIA V100 GPUs. The training process can alternatively be performed on a single GPU. To achieve this, the Maximum Inner Product Search is performed on the CPU using the FAISS library. This allows the document index vectors to be stored on the CPU, requiring approximately 100 GB of memory for the entire Wikipedia dataset. \n",
      "\n",
      "For evaluation purposes, test numbers are reported using ten retrieved documents for both RAG-Token and RAG-Sequence models. To compare with a baseline, a BART-large model is also trained. For the RAG-Sequence model, a beam size of four is used, and the Fast Decoding approach is applied. It's worth noting that the RAG model achieves state-of-the-art results on open-domain QA without the need for expensive specialized pre-training, such as salient span masking, unlike some of its predecessors. \n",
      "\n",
      "The advantages of the RAG model over extraction-based approaches include leveraging documents that contain clues about the answer even if they don't provide the answer directly. This allows the model to generate more comprehensive and accurate responses. \n",
      "\n",
      "Would you like me to go over the parameters of the RAG model?\n"
     ]
    }
   ],
   "source": [
    "request_body = {\n",
    "    \"userprompt\": \"Can you explain the training procedure?\"\n",
    "}\n",
    "response = requests.post(url, json=request_body)\n",
    "print(response.json())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T21:51:25.491653300Z",
     "start_time": "2023-12-31T21:51:14.206578400Z"
    }
   },
   "id": "d2c3773624130c75"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The retrieval collapse experiments in the paper you referenced were conducted to observe and analyze the behavior of the retrieval component in question-answering tasks, specifically in story generation. Retrieval collapse refers to the situation where the model learns to retrieve the same documents for every input, regardless of their content. This behavior can limit the effectiveness of the model in generating diverse and accurate responses.\n",
      "\n",
      "The authors conducted experiments using a dataset specific to story generation, and they observed that the retrieval component of the model tended to converge on a small set of documents for all stories. This resulted in the generator learning to ignore the retrieved documents, leading to equivalent performance compared to a baseline model without retrieval (BART). \n",
      "\n",
      "They suggest that this collapse could be due to less emphasis on factual knowledge or longer target sequences in story generation. The authors also cite previous work by Perez et al., which had similar findings regarding spurious retrieval results when optimizing retrieval components.\n",
      "\n",
      "Overall, the retrieval collapse experiments highlight the importance of evaluating and understanding the behavior of retrieval-based models to ensure their effectiveness in generating relevant and diverse responses. \n",
      "\n",
      "Would you like me to explain any of the details of the experiment, or provide more information on related work?\n"
     ]
    }
   ],
   "source": [
    "request_body = {\n",
    "    \"userprompt\": \"What are the retrieval collapse experiments in the paper?\"\n",
    "}\n",
    "response = requests.post(url, json=request_body)\n",
    "print(response.json())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T21:53:39.931652700Z",
     "start_time": "2023-12-31T21:53:29.260050400Z"
    }
   },
   "id": "e827a81f847acf01"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The experiments that evaluate open-domain question answering in the provided paper are listed below:\n",
      "\n",
      "1. Natural Questions\n",
      "2. TriviaQA\n",
      "3. WebQuestions\n",
      "4. CuratedTrec\n",
      "\n",
      "The methods for these experiments utilize a model initialized with retrieved information from a single Wikipedia dump, and is then trained to retrieve the top documents for each query.\n",
      "\n",
      "Would you like me to go into more detail about any of these experiments?\n"
     ]
    }
   ],
   "source": [
    "request_body = {\n",
    "    \"userprompt\": \"List all the open domain question answering experiments performed in the paper?\"\n",
    "}\n",
    "response = requests.post(url, json=request_body)\n",
    "print(response.json())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T21:56:10.769453Z",
     "start_time": "2023-12-31T21:56:06.008021200Z"
    }
   },
   "id": "ebfd64bb5ad92e90"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model performs within 4.3% of the state of the art on the Fact Verification task. The model uses the input sequence to retrieve text documents and uses that information to generate a target sequence. For the Fact Verification task, the model achieves results that are highly factual and specific. Would you like to know more about the model's performance on other tasks?\n"
     ]
    }
   ],
   "source": [
    "request_body = {\n",
    "    \"userprompt\": \"How does the model perform on the Fact Verification task?\"\n",
    "}\n",
    "response = requests.post(url, json=request_body)\n",
    "print(response.json())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T21:58:06.907210500Z",
     "start_time": "2023-12-31T21:58:02.779496300Z"
    }
   },
   "id": "b6fc4bf7b46ddc6b"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the paper, the retrieval ablations performed involve freezing the retriever during training and comparing different types of retrievers, including dense retrievers based on word overlap and a BM25 (Biased Monte Carlo Term Frequency) retriever. They are compared to assess the effectiveness of the retrieval mechanism in the RAG model.\n",
      "\n",
      "Would you like help with understanding any of the other techniques introduced in the paper?\n"
     ]
    }
   ],
   "source": [
    "request_body = {\n",
    "    \"userprompt\": \"What are the retrieval ablations performed in the paper?\"\n",
    "}\n",
    "response = requests.post(url, json=request_body)\n",
    "print(response.json())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T21:59:06.150676600Z",
     "start_time": "2023-12-31T21:59:01.820100700Z"
    }
   },
   "id": "8ef35348b3fa210f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
